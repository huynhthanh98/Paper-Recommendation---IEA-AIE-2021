{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"function.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"qA_fHTUSixIb"},"source":["# standard\n","import numpy as np\n","import pandas as pd\n","\n","# visualize\n","from tqdm.notebook import tqdm\n","from itertools import product\n","\n","# system\n","import pickle     ## saving library\n","import os         ## file manager\n","import sys\n","from multiprocessing import Pool\n","import time\n","\n","import re         ## preprocessing text library\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')     # download toolkit for textblob.TextBlob.words\n","\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer     # tranform expanding words of words like attacker, attacked, attacking -> attack\n","st = PorterStemmer()\n","\n","stop_words = stopwords.words('english')\n","stop = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"isnt\", \"it\", \"its\", \"itself\", \"keep\", \"keeps\", \"kept\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"names\", \"named\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"ok\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"puts\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"sees\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"shows\", \"showed\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n","\n","\n","class preprocess_tool():\n","    def __init__(self, tool_preprocessText):\n","        self.tool_preprocessText = tool_preprocessText\n","\n","    def get_preprocessed_data(self, dataframe, \n","                              preprocess_columns = ['title', 'abstract', 'keywords'],\n","                              keep_columns = ['itr'],\n","                              new_column_names = ['Title', 'Abstract', 'Keywords'],\n","                              n_jobs=4):\n","        '''-Parameters:\n","              preprocess_columns: ['title', 'abstract', 'keywords']\n","              keep_columns: ['itr']\n","              new_column_names:\n","           -Return:\n","              (pandas DataFrame): data after preprocess\n","        '''########\n","        output_data = pd.DataFrame(columns=new_column_names)\n","        output_data[new_column_names] = dataframe[preprocess_columns + keep_columns]\n","\n","        if 'Title' in new_column_names:\n","            with Pool(n_jobs) as p:\n","                output_data['Title'] = p.map(self.tool_preprocessText.preprocess_Title, output_data['Title'])\n","        if 'Abstract' in new_column_names:\n","            with Pool(n_jobs) as p:\n","                output_data['Abstract'] = p.map(self.tool_preprocessText.preprocess_Abstract, output_data['Abstract'])\n","        if 'Keywords' in new_column_names:\n","            with Pool(n_jobs) as p:\n","                output_data['Keywords'] = p.map(self.tool_preprocessText.preprocess_Keywords, output_data['Keywords'])\n","        if \"Aims\" in new_column_names:\n","            with Pool(n_jobs) as p:\n","                output_data['Aims'] = p.map(self.tool_preprocessText.preprocess_Aims, output_data['Aims'])\n","        return output_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yw6tpiZ12rT"},"source":["def labelling_data(series, category):\n","    '''-Parameter:\n","          series(pandas Series): Conference distribution of data.\n","          category(Int64Index list): category (do not reset_index of aims_content before using this funtion)\n","        -Return: \n","          (np array): label series for data.\n","    '''########\n","    label = np.zeros(len(series))\n","    for i, j in enumerate(category):\n","        label[series == j] = i\n","    return label.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G13wwZRAyN4p"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import io\n","\n","class NLP_tool():\n","    def __init__(self):\n","        pass\n","\n","    def build_tokenizer(self, contents, vocabulary_size = None):\n","        '''-Parameters:\n","              contents(list): list of texts or strings.\n","           -Return: tokenizer object.\n","        '''######\n","        tokenizer = Tokenizer(num_words = vocabulary_size)\n","        tokenizer.fit_on_texts(contents)\n","        return tokenizer\n","\n","    def tokenize_data(self, Series, tokenizer, maxlen=None):\n","        '''- Parameters:\n","              Series(pandas DataFrame Series): list of strings\n","              tokenizer(keras Tokenizer)\n","              maxlen(int): max length tokenkize\n","           - Return: \n","              (numpy array): tokenized matrix of input Series\n","        '''######\n","        contents = Series.tolist()\n","        sequences = tokenizer.texts_to_sequences(contents)\n","        if maxlen == None:\n","            data = pad_sequences(sequences)\n","        else:\n","            data = pad_sequences(sequences, maxlen=maxlen)\n","        return data\n","\n","    def download_FastText_pretrained(self):\n","        if os.path.exists('crawl-300d-2M.vec'):\n","            print(\"File have already downloaded.\")\n","            return\n","        !wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'\n","        !unzip 'crawl-300d-2M.vec.zip'\n","        os.remove('crawl-300d-2M.vec.zip')\n","\n","    def download_Wiki_pretrained(self):\n","        if os.path.exists('wiki.en.vec'):\n","            print(\"File have already downloaded.\")\n","            return\n","        !wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec'\n","\n","    def build_fasttext_embedding_dict(self):\n","        fasttext_embedding_dict = dict()\n","        with io.open('crawl-300d-2M.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n","            headline = f.readline()\n","            n_fasttext, ed_fasttext = np.array(headline.split(), dtype=int)\n","            for line_idx in tqdm(range(n_fasttext)):\n","                line = f.readline()\n","                values = line.strip().rsplit(\" \")\n","                word = values[0]\n","                if word.encode().isalpha():     # avoid iconology word like 茶道, приглашаю, مقاولات, ハイキュー, 유형,...\n","                    if word.islower():\n","                        fasttext_embedding_dict[word] = np.asarray(values[1:], dtype='float32')\n","                    else:\n","                        word = word.lower()\n","                        if word not in fasttext_embedding_dict:\n","                            fasttext_embedding_dict[word] = np.asarray(values[1:], dtype='float32')\n","        return fasttext_embedding_dict\n","\n","    def build_embedding_matrix_FastText(self, tokenizer, embedding_dict):\n","        print(\"Building embedding matrix .....\")\n","        if tokenizer.num_words == None:\n","            vocabulary_size = len(tokenizer.word_index)\n","        else:\n","            vocabulary_size = tokenizer.num_words\n","\n","        zero_tokens = dict()\n","        embedding_matrix = np.zeros((vocabulary_size + 1, 300))\n","\n","        for i in tqdm(range(1,vocabulary_size+1)):\n","            word = tokenizer.index_word[i]\n","            try:\n","                vec = embedding_dict[word]\n","                embedding_matrix[i] = vec\n","            except:\n","                zero_tokens[i] = word\n","        \n","        return embedding_matrix, zero_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwRMbsE27x-a"},"source":["from scipy.spatial import distance\n","from scipy import sparse\n","\n","class similarity_tool():\n","    def __init__(self,embedding_matrix, zero_tokens,\n","                 vocabulary_size = 50000, embedding_dimension=300\n","                 ):\n","        self.embedding_matrix = embedding_matrix\n","        self.embedding_dimension = embedding_dimension\n","        self.zero_tokens = zero_tokens\n","    \n","    def calculate_centroid_data(self, tokenized_data):\n","        '''-Parameter:\n","              tokenized_data(numpy array):\n","           -Return:\n","              (numpy array): matrix A each row is centroid vector respective tokenized_data points. \n","        '''######\n","        # nPoints = tokenized_data.shape[0]\n","        # embedding_dimension = self.embedding_matrix.shape[1]\n","        \n","        with Pool(4) as p:\n","            centroid_data = p.map(self.calculate_centroid_vector, tokenized_data)\n","        centroid_data = np.array(centroid_data)\n","        return centroid_data\n","\n","    # def calculate_centroid_vector(self, tokenized_doc):\n","    #     tokens = tokenized_doc[tokenized_doc != 0]\n","    #     if len(tokens):\n","    #         centroid_vector = np.mean(self.embedding_matrix[tokens], axis=0)\n","    #     else:\n","    #         centroid_vector = np.zeros(self.embedding_matrix.shape[1])\n","    #     return centroid_vector\n","\n","    def calculate_centroid_vector(self, tokenized_doc):\n","        tokens = tokenized_doc[tokenized_doc != 0]\n","        tokens = [token for token in tokens if token not in self.zero_tokens]\n","        if len(tokens):\n","            centroid_vector = np.mean(self.embedding_matrix[tokens], axis=0)\n","        else:\n","            centroid_vector = np.zeros(self.embedding_matrix.shape[1])\n","        return centroid_vector\n","\n","    def calculate_matrix_cosine_similarity(self, centroid_data_01, centroid_data_02):\n","        '''-Parameter:\n","              centroid_data_01(numpy array):\n","              centroid_data_02(numpy array):\n","           -Return:\n","              (numpy array): matrix A(i,j) which (i,j) is cosine distance of data_(i)th and aims_(j)th \n","        '''######\n","        \n","        nrow = centroid_data_01.shape[0]\n","        ncol = centroid_data_02.shape[0]\n","\n","        A = np.zeros([nrow, ncol])\n","        for col in tqdm(range(ncol)):\n","            if np.count_nonzero(centroid_data_02[col]):\n","                for row in range(nrow):    \n","                    A[row, col] = distance.cosine(centroid_data_01[row], [centroid_data_02[col]])\n","        return A\n","    \n","    def init_cosine_matrix_aims(self, centroid_data_aims):\n","        self.centroid_data_aims = centroid_data_aims\n","        self.n_aims = centroid_data_aims.shape[0]\n","\n","    def calculate_similarity_vector(self, vector_centroid_x):\n","        output_vector = np.zeros(self.n_aims)\n","        if np.count_nonzero(vector_centroid_x):\n","            for i in range(self.n_aims):\n","                output_vector[i] = distance.cosine(vector_centroid_x, self.centroid_data_aims[i])\n","        return output_vector\n","\n","    def calculate_cosine_similarity_with_aims(self, centroid_data_01):\n","        with Pool(4) as p:\n","            output_matrix = p.map(self.calculate_similarity_vector, centroid_data_01)\n","        output_matrix = np.asarray(output_matrix)\n","        return output_matrix\n","         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoVL5G_qRApo"},"source":["print(3)"],"execution_count":null,"outputs":[]}]}